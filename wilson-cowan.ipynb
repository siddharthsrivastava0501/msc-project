{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WilsonCowanParams:\n",
    "    def __init__(self):\n",
    "        self.r = 0.2 \n",
    "        self.tau_E = 1.0\n",
    "        self.tau_I = 2.0\n",
    "        self.w_ee = 8 + np.random.randn()\n",
    "        self.w_ei = 4 + np.random.randn()\n",
    "        self.w_ie = 10 + np.random.randn()\n",
    "        self.w_ii = 3 + np.random.randn()\n",
    "        self.P = 1 + 0.1 * np.random.randn()\n",
    "        self.Q = 1 + 0.1 * np.random.randn()\n",
    "\n",
    "class Region:\n",
    "    def __init__(self, region_id, T, dt):\n",
    "        t = np.arange(0, T + dt, dt)\n",
    "        self.region_id = region_id\n",
    "        self.E = np.zeros(len(t))\n",
    "        self.I = np.zeros(len(t))\n",
    "\n",
    "        self.E[0] = 0.2\n",
    "        self.I[0] = 0.3\n",
    "\n",
    "        self.params = WilsonCowanParams()\n",
    "\n",
    "        \n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dedt(t, E, I, E_ext, region : Region, C):\n",
    "    params = region.params\n",
    "    print(np.dot(C[region.region_id], E_ext), params.w_ee*E)\n",
    "    return (-E + (1 - params.r*E) * Se(params.w_ee*E + G * np.dot(C[region.region_id], E_ext) - params.w_ei*I + params.P)) / params.tau_E\n",
    "\n",
    "def didt(t, E, I,region):\n",
    "    params = region.params\n",
    "    return (-I + (1 - params.r*I) * Si(params.w_ie*E - params.w_ii*I + params.Q)) / params.tau_I\n",
    "\n",
    "def plot_results(regions, t):\n",
    "    fig, axs = plt.subplots(10, 10, figsize=(60, 60))\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for i, region in enumerate(regions):  # Plot first 4 regions\n",
    "        axs[i].plot(t, region.E, label='E')\n",
    "        axs[i].plot(t, region.I, label='I')\n",
    "        axs[i].set_title(f'Region {region.region_id}')\n",
    "        axs[i].set_xlabel('Time')\n",
    "        axs[i].set_ylabel('Activity')\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main simulation\n",
    "num_regions = 100\n",
    "T = 100\n",
    "dt = 0.01\n",
    "G = 2.\n",
    "  \n",
    "# Load and normalise connectivity matrix\n",
    "C = pd.read_csv('./fmri/DTI_fiber_consensus_HCP.csv', header=None).to_numpy()[:num_regions, :num_regions]\n",
    "np.fill_diagonal(C, 0)\n",
    "C /= C.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Run simulation\n",
    "t = np.arange(0, T + dt, dt)\n",
    "regions = [Region(i, T, dt) for i in range(num_regions)]\n",
    "\n",
    "for i in range(len(t) - 1):\n",
    "    E_ext = np.array([r.E[i] for r in regions])\n",
    "    \n",
    "    for r in range(num_regions):\n",
    "        curr = regions[r]\n",
    "        \n",
    "        curr.E[i + 1] = curr.E[i] + dt * (dedt(t[i], curr.E[i], curr.I[i], E_ext, curr, C))\n",
    "        curr.I[i + 1] = curr.I[i] + dt * (didt(t[i], curr.E[i], curr.I[i], curr))\n",
    "\n",
    "# Plot results\n",
    "plot_results(regions, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('animation', html='jshtml')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# constants\n",
    "r = .2\n",
    "tau_E = 1.\n",
    "tau_I = 2.\n",
    "\n",
    "P = 0.2\n",
    "Q = .5\n",
    "\n",
    "w_ee = 10\n",
    "w_ei = 12\n",
    "w_ie = 9\n",
    "w_ii = 3\n",
    "\n",
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return 2*sig(2*x) - 1\n",
    "\n",
    "def dedt(t, E, I, P):\n",
    "    de = (-E + (1 - r*E)*sig(w_ee*E - w_ei*I + P)) / tau_E\n",
    "    # de = (-E + sig(k*E + k*P)) / tau_E\n",
    "    return de\n",
    "\n",
    "def didt(t, E, I, Q):\n",
    "    di = (-I + (1 - r*I)*sig(w_ie*E - w_ii*I + Q)) / tau_I\n",
    "    return np.array([di])\n",
    "\n",
    "T = 6\n",
    "dt = .01\n",
    "t = np.arange(0, T + dt, dt)\n",
    "E = np.zeros(len(t))\n",
    "I = np.zeros(len(t))\n",
    "\n",
    "for i in range(len(t) - 1):\n",
    "    E[i + 1] = E[i] + dt * dedt(t[i], E[i], I[i], P)\n",
    "    I[i + 1] = I[i] + dt * didt(t[i], E[i], I[i], Q)\n",
    "\n",
    "# E += np.random.normal(0, 0.01, E.shape)\n",
    "plt.plot(E, label='Excitatory')\n",
    "plt.plot(I, label='Inhibitory')\n",
    "# plt.plot(I, label='Inhibitory')\n",
    "plt.legend()\n",
    "\n",
    "# np.save('E_synthetic_kdot5_Pdot2.npy', E)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Activity')\n",
    "# plt.savefig('varying_k.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import norm\n",
    "\n",
    "N    = 200\n",
    "X    = np.linspace(-4, 4, N)\n",
    "Y    = np.linspace(-4, 4, N)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "pos  = np.dstack((X, Y))\n",
    "\n",
    "\n",
    "def gauss2d(mean_a, mean_b, cov_aa, cov_bb):\n",
    "    rv = multivariate_normal([mean_a, mean_b], [[cov_aa, 0.8], [0.8, cov_bb]])\n",
    "    Z = rv.pdf(pos)\n",
    "    plt.contour(X, Y, Z)\n",
    "    plt.show()\n",
    "\n",
    "def gauss1d(mean, std):\n",
    "    X = np.linspace(-4, 4, N)\n",
    "    z = norm.pdf(X, mean, std)\n",
    "    plt.plot(X, z)\n",
    "    plt.show()\n",
    "    \n",
    "# interact(gauss2d, mean_a = widgets.FloatSlider(value=0, min=0.1, max=5, step=0.1),\n",
    "#     mean_b = widgets.FloatSlider(value=0, min=0.1, max=5, step=0.1),\n",
    "#     cov_aa = widgets.FloatSlider(value=1, min=0.1, max=5, step=0.1),\n",
    "#     cov_bb = widgets.FloatSlider(value=2, min=0.1, max=5, step=0.1))\n",
    "\n",
    "interact(gauss1d, mean = widgets.FloatSlider(value = 0, min = 0, max = 5, step=0.1), std = widgets.FloatSlider(value = 0, min = .1, max = 3, step = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Et_slider = widgets.FloatSlider(value = 0, min = -5, max = 5, step=0.1)\n",
    "\n",
    "def gauss1d_neuronal(Et, std):\n",
    "    X = np.linspace(-4, 4, N)\n",
    "    mean = Et + dt*dedt(0, Et, I=0, P=.2)\n",
    "    print(Et, mean)\n",
    "    z = norm.pdf(X, mean, std)\n",
    "    plt.plot(X, z)\n",
    "    plt.show()\n",
    "\n",
    "def gauss2d_neuronal(mean_a, mean_b, cov_aa, cov_bb):\n",
    "    rv = multivariate_normal([mean_a, mean_b], [[cov_aa, 0.8], [0.8, cov_bb]])\n",
    "    Z = rv.pdf(pos)\n",
    "    plt.contour(X, Y, Z)\n",
    "    plt.show()\n",
    "\n",
    "interact(gauss1d_neuronal, Et = Et_slider, std = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "P = .2\n",
    "sigma_n = 0.05\n",
    "k = 5\n",
    "tau_E = 1\n",
    "\n",
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def dedt(t, E, I, P):\n",
    "    # de = (-E + (1 - r*E)*sig(w_ee*E - w_ei*I + P)) / tau_E\n",
    "    de = (-E + sig(k*E + k*P)) / tau_E\n",
    "    return de\n",
    "\n",
    "def ddedt(E):\n",
    "    s =  sig(k*E + k*P)\n",
    "    dde = (-1 + k * s * (1 - s)) / tau_E\n",
    "    return dde\n",
    "\n",
    "def j_func(E):\n",
    "    return 1 + ddedt(E)\n",
    "\n",
    "def Etp(E):\n",
    "    return E + dedt(0, E, 0, P)\n",
    "\n",
    "def linearised(x0):\n",
    "    J = j_func(X)\n",
    "    return Etp(x0) + J*(X - x0)\n",
    "\n",
    "def gauss_approx(x0):\n",
    "    J_x0 = j_func(x0)\n",
    "    sigman_inv = (1/sigma_n)\n",
    "    eta = J_x0.T * (z(x0) - Etp(x0) + J_x0*x0)\n",
    "    lam = J_x0.T * sigman_inv * J_x0\n",
    "\n",
    "    return eta, lam\n",
    "\n",
    "X = np.linspace(-1, 1, 500)\n",
    "\n",
    "def z(X):\n",
    "    X = np.asarray(X)\n",
    "    return Etp(X) + np.random.normal(0, 0.05, size=X.shape)\n",
    "\n",
    "def linearised_with_slider(x0):\n",
    "    plt.plot(X, z(X))\n",
    "    plt.plot(X, linearised(x0))\n",
    "    plt.axvline(x = x0, color = 'b', label = 'axvline - full height')\n",
    "    plt.show()\n",
    "\n",
    "X = np.linspace(-1, 1, 500)\n",
    "x0 = 0.5\n",
    "\n",
    "interact(linearised_with_slider, x0 = widgets.FloatSlider(value = 0, min = -1, max = 1, step=0.1))\n",
    "\n",
    "# def gauss1d(mean, std):\n",
    "#     z = norm.pdf(np.linalg.norm(-5, 5, 500), mean, std)\n",
    "#     plt.plot(np.linalg.norm(-5, 5, 500), z)\n",
    "#     plt.show()\n",
    "\n",
    "# def inf_to_moments(eta, lam):\n",
    "#     return (eta/lam, 1/lam)\n",
    "\n",
    "# gauss1d(*inf_to_moments(*gauss_approx(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "\n",
    "tau_E = 1.\n",
    "P = .2\n",
    "k = 5\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "def _dedt(E):\n",
    "    return (-E + sigmoid(k*E + k*P)) / tau_E\n",
    "\n",
    "def meas_fn(Et):\n",
    "    return Et + _dedt(Et)\n",
    "\n",
    "def jac_fn(Et):\n",
    "    meas_fn(Et).backward()\n",
    "    return Et.grad\n",
    "\n",
    "x0 = 0.2\n",
    "x_pos = torch.as_tensor(x0).requires_grad_(True)\n",
    "MEAS = meas_fn(x_pos)\n",
    "meas_lam = torch.tensor([1/0.1])\n",
    "# pred_meas = 0.8\n",
    "\n",
    "pred_meas = torch.linspace(0.5, 1.5, 10)\n",
    "\n",
    "diff = MEAS - pred_meas\n",
    "    \n",
    "print(torch.exp(-0.5 * diff * meas_lam * diff.T).detach().numpy())\n",
    "\n",
    "jac = jac_fn(x_pos)\n",
    "\n",
    "diff = MEAS - pred_meas\n",
    "lam = jac.T * meas_lam * jac\n",
    "eta = jac.T * meas_lam * (diff + jac * x_pos)\n",
    "\n",
    "print(torch.exp(-0.5 * MEAS * lam * MEAS - eta * MEAS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "\n",
    "k = 5\n",
    "P = .2\n",
    "tau_E = 1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "def _dedt(E):\n",
    "    return (-E + sigmoid(k*E + k*P)) / tau_E\n",
    "\n",
    "def meas_fn(Et, Etp):\n",
    "    Etp, Et = torch.as_tensor(Etp), torch.as_tensor(Et)\n",
    "    return torch.abs(Etp - (Et + _dedt(Et)))\n",
    "\n",
    "Et = torch.tensor(0.2, requires_grad=True)\n",
    "Etp = torch.tensor(0.8, requires_grad=True)\n",
    "\n",
    "X0 = torch.tensor(([Et], [Etp]))\n",
    "\n",
    "sigma_n_inv = torch.tensor([[1/0.3, 0.], [0, 1/0.2]])\n",
    "\n",
    "Et, Etp = torch.tensor(Et, requires_grad=True), torch.tensor(Etp, requires_grad=True)\n",
    "h = meas_fn(Et, Etp)\n",
    "h.backward()\n",
    "\n",
    "J = torch.tensor((Et.grad, Etp.grad))\n",
    "eta = J.T @ sigma_n_inv * (J @ torch.tensor([Et, Etp]) - h) \n",
    "lam = J.T @ sigma_n_inv @ J\n",
    "\n",
    "Et_values = np.linspace(-2, 2, 100)\n",
    "Etp_values = np.linspace(-2, 2, 100)\n",
    "Et_grid, Etp_grid = np.meshgrid(Et_values, Etp_values)\n",
    "\n",
    "# Evaluate the function over the grid (vectorized)\n",
    "Et_tensor = torch.from_numpy(Et_grid)\n",
    "Etp_tensor = torch.from_numpy(Etp_grid)\n",
    "meas_values = meas_fn(Et_tensor, Etp_tensor).numpy()\n",
    "\n",
    "# Plot the contour plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "contour_levels = np.linspace(np.min(meas_values), np.max(meas_values), 100)\n",
    "cp = ax.contourf(Et_grid, Etp_grid, meas_values, levels=contour_levels, cmap='Greens')\n",
    "ax.plot(Et_values, (torch.tensor(Et_values) + _dedt(torch.tensor(Et_values))).numpy(), label='Line of optimality, h(Et, Etp) = 0')\n",
    "ax.set_xlabel('Et')\n",
    "ax.set_ylabel('Etp')\n",
    "cbar = fig.colorbar(cp)\n",
    "cbar.set_label('h(Et, Etp)')\n",
    "plt.title('h(Et, Etp) = |Etp - (Et + dedt(Et))|')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def inf_to_moments(eta, lam):\n",
    "    return (eta/lam, 1/lam)\n",
    "\n",
    "eta, lam\n",
    "inf_to_moments(eta, lam)\n",
    "\n",
    "print(f'True value: {meas_fn(Et, Etp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('animation', html='jshtml')\n",
    "\n",
    "colours = [\n",
    "    (0.0, 'red'),  \n",
    "    (0.5, 'white'),\n",
    "    (1.0, 'blue')  \n",
    "]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('RedWhiteBlue', colours)\n",
    "\n",
    "k = 5\n",
    "P = .2\n",
    "tau_E = 1\n",
    "\n",
    "lmbda_in = torch.tensor([[0.05 ** -2]])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "def _dedt(E):\n",
    "    return (-E + sigmoid(k*E + k*P)) / tau_E\n",
    "\n",
    "def meas_fn(Et, Etp):\n",
    "    Etp, Et = torch.as_tensor(Etp), torch.as_tensor(Et)\n",
    "    return torch.abs(Etp - (Et + _dedt(Et)))\n",
    "\n",
    "Et, Etp = 0.2, 0.8\n",
    "Et_values = torch.linspace(-2, 2, 1000)\n",
    "Etp_values = torch.linspace(-2, 2, 1000)\n",
    "\n",
    "Et_tensor = torch.tensor(Et, requires_grad=True)\n",
    "Etp_tensor = torch.tensor(Etp, requires_grad=True)\n",
    "\n",
    "X0 = torch.tensor((Et_tensor, Etp_tensor))\n",
    "h_X0 = meas_fn(Et_tensor, Etp_tensor)\n",
    "h_X0.backward()\n",
    "J = torch.tensor([[Et_tensor.grad, Etp_tensor.grad]])\n",
    "# print(h_X0, J)\n",
    "\n",
    "print(f'Using {Et} and {Etp} to calculate...')\n",
    "\n",
    "eta = J.T @ lmbda_in * ((J @ X0) - h_X0) \n",
    "lmbdap = J.T @ lmbda_in @ J\n",
    "\n",
    "Et_grid, Etp_grid = torch.meshgrid(Et_values, Etp_values)\n",
    "\n",
    "X_flat = torch.stack((Etp_grid.flatten(), Et_grid.flatten()), dim=1)\n",
    "\n",
    "# print((X_flat - X0).shape)\n",
    "h_approx_flat = h_X0 + J @ (X_flat - X0).T\n",
    "\n",
    "meas_values = h_approx_flat.detach().view(Et_grid.shape).numpy()\n",
    "\n",
    "plt.plot(Et, Etp, 'r.')\n",
    "contour_levels = np.linspace(-3.8, 3.8, 200)\n",
    "# # print(f'True Value: {meas_fn(0.2, 0.8)}, Approximated value: {h_approx(0.2, 0.8, 0.2, 0.8)}')\n",
    "plt.contourf(Et_values, Etp_values, meas_values, levels=contour_levels, cmap=cmap)\n",
    "plt.xlabel(r'$E_t$')\n",
    "plt.ylabel(r'$E_{t+1}$')\n",
    "plt.title(rf'$h(X) \\approx h(X_0) + J(X - X_0), X_0 = [{Et}, {Etp}]$')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('$h(E_t, E_{t + 1}$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "from scipy.optimize import fsolve\n",
    "from fg.functions import dEdt, dIdt\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "config = {\n",
    "    'T': 10,\n",
    "    'dt': 0.01,\n",
    "    'k1': 16,\n",
    "    'k2': 12,\n",
    "    'k3': 15,\n",
    "    'k4': 3,\n",
    "    'P': 0.2,\n",
    "    'Q': 0.2,\n",
    "    'tau_E': 1.,\n",
    "    'tau_I': 1.\n",
    "}\n",
    "\n",
    "T = config.get('T', 6.)\n",
    "dt = config.get('dt', 0.01)\n",
    "k1 = config.get('k1', 10.)\n",
    "k2 = config.get('k2', 12.)\n",
    "k3 = config.get('k3', 9.)\n",
    "k4 = config.get('k4', 3.)\n",
    "P = config.get('P', 0.2)\n",
    "Q = config.get('Q', 0.5)\n",
    "tauE = config.get('tauE', 1.)\n",
    "tauI = config.get('tauI', 2.)\n",
    "\n",
    "time = torch.arange(0, T + dt, dt)\n",
    "\n",
    "E0, I0 = 0.4, 0.5\n",
    "E = torch.zeros(len(time))\n",
    "I = torch.zeros(len(time))\n",
    "\n",
    "E[0] = E0\n",
    "I[0] = I0\n",
    "\n",
    "for i in range(len(time) - 1):\n",
    "    E[i + 1] = E[i] + dt * dEdt(E[i], I[i], k1, k2, P, tauE)\n",
    "    I[i + 1] = I[i] + dt * dIdt(E[i], I[i], k3, k4, Q, tauI)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10.3,3))\n",
    "plt.ylabel(r'$E, I$')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.plot(time, E, '.-', label=\"excitatory\");\n",
    "plt.plot(time, I, '.-', label=\"inhibitory\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fg.simulation_config import simulate_wc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "config = {\n",
    "    'T': 9,\n",
    "    'dt': 0.01,\n",
    "    'k1': 4.77,\n",
    "    'k2': 4.907,\n",
    "    'k3': 0.4,\n",
    "    'k4': 3.6329,\n",
    "    'P': 1.,\n",
    "    'Q': 1.,\n",
    "}\n",
    "\n",
    "E, I = simulate_wc(config)\n",
    "plt.plot(E - I, label='GT_E')\n",
    "# plt.plot(I, label='GT_I')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation with: T = 0.04, dt = 0.01, k1 = 2.992554188416303, k2 = 5.140534962181106, k3 = 4.848443103889046, k4 = 1.3618461573732632, P = 1.041932515681223, Q = 0.9671668189310363, tauE = 1.0, tauI = 2.0\n",
      "Iteration 0\n",
      "k1 Parameter p0 [n = 1, mu=tensor([[0.]]), cov=tensor([[4.]])]\n",
      "k2 Parameter p1 [n = 1, mu=tensor([[0.]]), cov=tensor([[4.]])]\n",
      "k3 Parameter p2 [n = 1, mu=tensor([[0.]]), cov=tensor([[4.]])]\n",
      "k4 Parameter p3 [n = 1, mu=tensor([[0.]]), cov=tensor([[4.]])]\n",
      "P Parameter p4 [n = 1, mu=tensor([[0.]]), cov=tensor([[4.]])]\n",
      "Q Parameter p5 [n = 1, mu=tensor([[0.]]), cov=tensor([[4.]])]\n",
      "------\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.inv: The diagonal element 2 is zero, the inversion could not be completed because the input matrix is singular.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr\u001b[38;5;241m.\u001b[39mright_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    106\u001b[0m fac \u001b[38;5;241m=\u001b[39m factor_graph\u001b[38;5;241m.\u001b[39mfactor_nodes[curr\u001b[38;5;241m.\u001b[39mright_id]\n\u001b[0;32m--> 107\u001b[0m \u001b[43mfac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_and_send_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MSc Project/fg/factors.py:317\u001b[0m, in \u001b[0;36mDynamicsFactor.compute_and_send_messages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_and_send_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, var_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected_vars):\n\u001b[0;32m--> 317\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_message_to_i\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39msend_msg_to_variable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor_id, var_id, msg)\n",
      "File \u001b[0;32m~/Documents/MSc Project/fg/factors.py:267\u001b[0m, in \u001b[0;36mDynamicsFactor._compute_message_to_i\u001b[0;34m(self, i, beta)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_message_to_i\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Gaussian:\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    Compute message to variable at index i in `self._vars`,\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    All of this is eqn 8 from 'Learning in Deep Factor Graphs with Gaussian Belief Propagation'\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     linearised_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinearise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     product \u001b[38;5;241m=\u001b[39m Gaussian\u001b[38;5;241m.\u001b[39mzeros_like(linearised_factor)\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# Build our message product by adding corresponding eta and lambda\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# in product\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc Project/fg/factors.py:223\u001b[0m, in \u001b[0;36mDynamicsFactor.linearise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m connected_variables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected_vars:\n\u001b[0;32m--> 223\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_var_belief\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m#nD beliefs\u001b[39;00m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mean\u001b[38;5;241m.\u001b[39mnumel()):\n",
      "File \u001b[0;32m~/Documents/MSc Project/fg/gaussian.py:33\u001b[0m, in \u001b[0;36mGaussian.mean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lmbda\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eta\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.inv: The diagonal element 2 is zero, the inversion could not be completed because the input matrix is singular."
     ]
    }
   ],
   "source": [
    "from fg.variables import Variable, Parameter\n",
    "from fg.factors import DynamicsFactor, ObservationFactor, PriorFactor\n",
    "from fg.functions import sig\n",
    "from fg.simulation_config import simulate_wc\n",
    "from fg.graph import Graph\n",
    "from fg.gaussian import Gaussian\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sigma_obs = 1e-2\n",
    "    sigma_dynamics = 1e-3\n",
    "    sigma_prior = 2e0\n",
    "    iters = 200\n",
    "\n",
    "    config = {\n",
    "        'T': 0.04,\n",
    "        'dt': 0.01,\n",
    "        'k1': 3. + np.random.normal(),\n",
    "        'k2': 5. + np.random.normal(),\n",
    "        'k3': 4. + np.random.normal(),\n",
    "        'k4': 3. + np.random.normal(),\n",
    "        'P': 1.  + np.random.normal(0, 0.1),\n",
    "        'Q': 1.  + np.random.normal(0, 0.1),\n",
    "    }\n",
    "\n",
    "    E, I = simulate_wc(config)\n",
    "    X = E - I\n",
    "    t = torch.arange(0, len(E), 1)\n",
    "\n",
    "    factor_graph = Graph()\n",
    "\n",
    "    # Add 0 as id as we populate them later\n",
    "    param_dict = {\n",
    "        'k1': Parameter(0, Gaussian(torch.tensor([[0.]]), torch.tensor([[sigma_prior ** 2.]])), factor_graph, []),\n",
    "        # 'ks': Parameter(0, Gaussian(torch.tensor([[0.] * 4]).T, torch.diag(torch.tensor([sigma_prior ** 2.] * 4))), factor_graph, [], 4),\n",
    "        'k2': Parameter(0, Gaussian(torch.tensor([[0.]]), torch.tensor([[sigma_prior ** 2.]])), factor_graph, []),\n",
    "        'k3': Parameter(0, Gaussian(torch.tensor([[0.]]), torch.tensor([[sigma_prior ** 2.]])), factor_graph, []),\n",
    "        'k4': Parameter(0, Gaussian(torch.tensor([[0.]]), torch.tensor([[sigma_prior ** 2.]])), factor_graph, []),\n",
    "        'P':  Parameter(0, Gaussian(torch.tensor([[0.]]), torch.diag(torch.tensor([sigma_prior ** 2.]))), factor_graph, [], 1),\n",
    "        'Q':  Parameter(0, Gaussian(torch.tensor([[0.]]), torch.diag(torch.tensor([sigma_prior ** 2.]))), factor_graph, [], 1)\n",
    "    }\n",
    "\n",
    "    # -- Construct FG -- #\n",
    "    # Add our variable and observation factors at each time step\n",
    "    for i in range(len(t)):\n",
    "        factor_graph.var_nodes[f'o{i}'] = Variable(f'o{i}', \n",
    "                                                   Gaussian(torch.tensor([[0.1, 0.1]]).T, torch.tensor([[0.2, 0.], [0., 0.2]])), \n",
    "                                                   -1 if i == 0        else (f'o{i-1}', f'o{i}'),\n",
    "                                                   -1 if i+1 == len(t) else (f'o{i}', f'o{i+1}'),\n",
    "                                                   -1, \n",
    "                                                   factor_graph, \n",
    "                                                   2)\n",
    "        \n",
    "        factor_graph.factor_nodes[f'o{i}'] = ObservationFactor(f'o{i}', f'o{i}', torch.tensor([[X[i]]]), torch.tensor([[sigma_obs ** -2]]), factor_graph)\n",
    "\n",
    "    # Add our parameters as additional variables to our factor graph\n",
    "    for p_id, (_,p) in enumerate(param_dict.items()):\n",
    "        p.id = f'p{p_id}'\n",
    "        factor_graph.param_ids.append(p.id)\n",
    "        factor_graph.var_nodes[p.id] = p\n",
    "\n",
    "    # Connect dynamics factors between timestep i and i+1 and connect each dyn. factor to our parameters\n",
    "    for i in range(len(t)):\n",
    "        if i+1 < len(t):\n",
    "            dyn_id = (f'o{i}', f'o{i+1}')\n",
    "            factor_graph.factor_nodes[dyn_id] = DynamicsFactor(f'o{i}', f'o{i+1}', torch.tensor([[sigma_dynamics ** -2]]), dyn_id, factor_graph)\n",
    "\n",
    "            for _,p in param_dict.items():\n",
    "                p.connected_factors.append(dyn_id)\n",
    "\n",
    "    # Zero mean priors on the parameters\n",
    "    for p_id, (_,p) in enumerate(param_dict.items()):\n",
    "        factor_graph.factor_nodes[f'p{p_id}'] = PriorFactor(f'p{p_id}', p.id, torch.tensor([[3.] * p.num_vars]).T, torch.diag(torch.tensor([sigma_prior ** -2] * p.num_vars)),\n",
    "                                                                  factor_graph)\n",
    "    \n",
    "    iter = 0\n",
    "    print(f'Iteration {iter}')\n",
    "    for k, v in param_dict.items():\n",
    "        print(k, v)\n",
    "\n",
    "        if v.belief.eta.isnan().any(): exit(0)\n",
    "\n",
    "    print('------')\n",
    "\n",
    "    if iter == 0:\n",
    "        # Initialise messages from observation factors to variables\n",
    "        # and prior factors to parameters (if learning params)\n",
    "        factor_graph.update_all_observational_factors()\n",
    "\n",
    "        # Now update messages from variables to factors\n",
    "        # This should ensure all var to dynamics factor messages have non-zero precision\n",
    "        for i in factor_graph.var_nodes:\n",
    "            curr = factor_graph.var_nodes[i]\n",
    "            curr.compute_and_send_messages()\n",
    "\n",
    "        factor_graph.prune()\n",
    "    \n",
    "    for i in range(len(t)):\n",
    "        curr = factor_graph.var_nodes[f'o{i}']\n",
    "        curr.compute_and_send_messages()\n",
    "\n",
    "        if curr.right_id == -1: continue\n",
    "\n",
    "        fac = factor_graph.factor_nodes[curr.right_id]\n",
    "        fac.compute_and_send_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
